{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain langchain-core langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define MariTalk API key and LLM model\n",
    "MARITALK_API_KEY = os.getenv('MARITALK_API_KEY')\n",
    "MARITALK_LLM_MODEL = \"sabia-3\"\n",
    "\n",
    "# Define Tavily API key\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct React Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet tavily-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.chat_models import ChatMaritalk\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "\n",
    "# Get the prompt from langchain hub\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "llm = ChatMaritalk(\n",
    "    model=MARITALK_LLM_MODEL,\n",
    "    api_key=MARITALK_API_KEY,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "# Construct the React agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Question: What is Langchain?\n",
      "Thought: To provide an accurate and comprehensive answer, I should first gather information about Langchain.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"What is Langchain?\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.ibm.com/think/topics/langchain', 'content': 'What Is LangChain? | IBM\\nWhat is LangChain?\\nArtificial Intelligence\\n31 October 2023\\nLink copied\\nWhat is LangChain?\\nLangChain is an open source orchestration framework for the development of applications using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and virtual agents. [...] LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert'}]\u001b[0m\u001b[32;1m\u001b[1;3m Question: What is Langchain?\n",
      "Thought: To provide an accurate and comprehensive answer, I should first gather information about Langchain.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"What is Langchain?\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.ibm.com/think/topics/langchain', 'content': 'What Is LangChain? | IBM\\nWhat is LangChain?\\nArtificial Intelligence\\n31 October 2023\\nLink copied\\nWhat is LangChain?\\nLangChain is an open source orchestration framework for the development of applications using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and virtual agents. [...] LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert'}]\u001b[0m\u001b[32;1m\u001b[1;3m Question: What is Langchain?\n",
      "Thought: To provide an accurate and comprehensive answer, I should first gather information about Langchain.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"What is Langchain?\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.ibm.com/think/topics/langchain', 'content': 'What Is LangChain? | IBM\\nWhat is LangChain?\\nArtificial Intelligence\\n31 October 2023\\nLink copied\\nWhat is LangChain?\\nLangChain is an open source orchestration framework for the development of applications using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and virtual agents. [...] LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert'}]\u001b[0m\u001b[32;1m\u001b[1;3m Question: What is Langchain?\n",
      "Thought: To provide an accurate and comprehensive answer, I should first gather information about Langchain.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"What is Langchain?\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.ibm.com/think/topics/langchain', 'content': 'What Is LangChain? | IBM\\nWhat is LangChain?\\nArtificial Intelligence\\n31 October 2023\\nLink copied\\nWhat is LangChain?\\nLangChain is an open source orchestration framework for the development of applications using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and virtual agents. [...] LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert'}]\u001b[0m\u001b[32;1m\u001b[1;3m Thought: Based on the repeated search results, I now have enough information to provide a final answer about Langchain.\n",
      "\n",
      "Final Answer: LangChain is an open source orchestration framework designed for the development of applications using large language models (LLMs). It provides tools and APIs in both Python and Javascript, making it easier to build applications driven by LLMs such as chatbots and virtual agents. LangChain simplifies the process by offering a library of abstractions for common steps and concepts needed when working with language models. Its modular components, such as functions and object classes, act as building blocks for generative AI programs. These components can be \"chained\" together to create applications, thereby reducing the amount of code and deep understanding required to perform complex natural language processing (NLP) tasks. Although LangChain's abstracted approach may somewhat limit the control an expert has over the finer details, it significantly lowers the barrier of entry for developing AI applications.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Langchain?',\n",
       " 'output': 'LangChain is an open source orchestration framework designed for the development of applications using large language models (LLMs). It provides tools and APIs in both Python and Javascript, making it easier to build applications driven by LLMs such as chatbots and virtual agents. LangChain simplifies the process by offering a library of abstractions for common steps and concepts needed when working with language models. Its modular components, such as functions and object classes, act as building blocks for generative AI programs. These components can be \"chained\" together to create applications, thereby reducing the amount of code and deep understanding required to perform complex natural language processing (NLP) tasks. Although LangChain\\'s abstracted approach may somewhat limit the control an expert has over the finer details, it significantly lowers the barrier of entry for developing AI applications.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is Langchain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['messages'] input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10c9d3920>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant!'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')]\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatMaritalk\n",
    "\n",
    "# Define the Maritalk model\n",
    "model = ChatMaritalk(\n",
    "    model=MARITALK_LLM_MODEL,\n",
    "    api_key=MARITALK_API_KEY,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant!\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "print(prompt_template)\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! It's great to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Define the configuration\n",
    "config = {\"configurable\": {\"thread_id\": \"thread_id_1\"}}\n",
    "query = \"Hi! I'm an undergraduate computer science student.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As a computer science student, your major would be in Computer Science. This typically covers a broad range of topics including programming, algorithms, data structures, computer architecture, and software engineering.\n"
     ]
    }
   ],
   "source": [
    "# Define the query\n",
    "query = \"What do I major in?\"\n",
    "\n",
    "# Invoke the agent\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet trimmer transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Hi there! I'm an undergraduate computer science student\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I am trying to learn about data structures and algorithms', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Data structures and algorithms are a fundamental part of computer science!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, trim_messages\n",
    "from langchain_community.chat_models import ChatMaritalk\n",
    "\n",
    "# Define the model\n",
    "model = ChatMaritalk(\n",
    "    model=MARITALK_LLM_MODEL,\n",
    "    api_key=MARITALK_API_KEY,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "# Define trimmer\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=1000,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "# Define the messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"),\n",
    "    HumanMessage(content=\"Hi there! I'm an undergraduate computer science student\"),\n",
    "    AIMessage(content=\"Hello! How can I help you today?\"),\n",
    "    HumanMessage(content=\"I am trying to learn about data structures and algorithms\"),\n",
    "    AIMessage(content=\"Data structures and algorithms are a fundamental part of computer science!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['messages'] input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10c9d3920>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={} messages=[MessagesPlaceholder(variable_name='messages')]\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "print(prompt_template)\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You mentioned that you are an undergraduate computer science student and that you're trying to learn about data structures and algorithms.\n"
     ]
    }
   ],
   "source": [
    "# Define the configuration\n",
    "config = {\"configurable\": {\"thread_id\": \"thread_id_2\"}}\n",
    "query = \"What was my previous interaction? What computer science concepts did I mention?\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
