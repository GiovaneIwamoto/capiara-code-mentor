{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docs to Read\n",
    "\n",
    "https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/use_cases/tool_use/prompting/\n",
    "\n",
    "https://python.langchain.com/docs/how_to/tools_model_specific/\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/qa_chat_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "%pip install --quiet langchain-ollama langchain-pinecone\n",
    "%pip install --quiet pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define MariTalk API key and LLM model\n",
    "MARITALK_API_KEY = os.getenv('MARITALK_API_KEY')\n",
    "MARITALK_LLM_MODEL = \"sabia-3\"\n",
    "\n",
    "# Define Langsmith API key and tracing\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "LANGSMITH_TRACING = os.getenv('LANGSMITH_TRACING')\n",
    "\n",
    "# Define Pinecone API key\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Define Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Pinecone database index\n",
    "index_name = \"nomic-embed-text-capiara-algorithm-mentor\"\n",
    "\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Initialize Pinecone vector store\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load a PDF document\n",
    "document_to_load = \"../docs/normas_atividade_extensao.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(document_to_load)\n",
    "docs = loader.load()\n",
    "\n",
    "# Chunk the document into smaller pieces\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split into {len(all_splits)} sub-documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Running this cell is RAM intensive consuming depending on the embedding model, please consider using a smaller model.\n",
    "# Remember to run Ollama in the background and make sure the model is downloaded.\n",
    "\n",
    "# Index chunks\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(f\"Document IDs: {document_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatMaritalk\n",
    "\n",
    "# Define the tool schema for the LLM\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"retrieve\",\n",
    "            \"description\": \"Retrieve relevant documents from the knowledge base given a user query.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The user's question or query to search in the vector database.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# MariTalk LLM initialization\n",
    "llm = ChatMaritalk(\n",
    "    model=MARITALK_LLM_MODEL,\n",
    "    api_key=MARITALK_API_KEY,\n",
    "    max_tokens=10000,\n",
    "    temperature=0.2,\n",
    "    tools=tools_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "# Define the state for the graph\n",
    "class MessagesState(TypedDict):\n",
    "    messages: List\n",
    "\n",
    "\n",
    "# Tool to retrieve documents from the vector store\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve relevant documents from the vector store based on a user question.\"\"\"\n",
    "    print(f\"üîç [TOOL] Calling 'retrieve' with query: {query}\")\n",
    "    \n",
    "    retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "    print(f\"üìö [TOOL] Documents found: {len(retrieved_docs)}\")\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"    üìò Doc {i+1}: {doc.page_content[:80]}...\")\n",
    "    \n",
    "    serialized = \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata}\\nContent: {doc.page_content}\"\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "# Parse the tool call from the LLM response\n",
    "def parse_tool_call(response):\n",
    "    \"\"\"\n",
    "    Attempts to convert response.content into JSON and detect the \"tool_call\" key.\n",
    "    If found, converts the keys 'function' to 'name' and 'arguments' to 'args',\n",
    "    and injects a unique \"id\" (if it doesn't exist) for compatibility with the ToolNode.\n",
    "    Returns a dictionary with tool data or None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content = response.content.strip()\n",
    "        parsed = json.loads(content)\n",
    "        if \"tool_call\" in parsed:\n",
    "            call = parsed[\"tool_call\"]\n",
    "            \n",
    "            # Convert 'function' to 'name'\n",
    "            if \"function\" in call:\n",
    "                call[\"name\"] = call.pop(\"function\")\n",
    "            \n",
    "            # Convert 'arguments' to 'args'\n",
    "            if \"arguments\" in call:\n",
    "                call[\"args\"] = call.pop(\"arguments\")\n",
    "            \n",
    "            # Inject a unique id if it doesn't exist\n",
    "            if \"id\" not in call:\n",
    "                call[\"id\"] = str(uuid.uuid4())\n",
    "            return call\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"‚õîÔ∏è [ERROR] Error converting to JSON:\", e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "# Decide whether to call the tool or respond directly\n",
    "def query_or_respond(state: MessagesState):\n",
    "    #FIXME: Better system instructions prompt\n",
    "    system_instructions = (\"\"\"\n",
    "        You are a helpful assistant with access to a specialized document database containing information related to university files and related subjects.\n",
    "        ONLY call the tool 'retrieve' (by returning a JSON object as specified below) if the user's query is clearly about this domain.\n",
    "        If the user's query is about general topics or subjects not related to this domain, answer directly without calling any tool. \\n\n",
    "        When calling the tool, respond ONLY with a JSON object in the following format and NOTHING else:\\n\n",
    "        {\"tool_call\": {\"function\": \"retrieve\", \"arguments\": {\"query\": \"<your query>\"}}}\\n\n",
    "        If no external specialized information is required, answer directly.\n",
    "    \"\"\")\n",
    "    # Prompt the LLM with system instructions and the conversation history\n",
    "    new_messages = [SystemMessage(system_instructions)] + state[\"messages\"]\n",
    "    \n",
    "    # Invoke the LLM with the new messages\n",
    "    print(\"ü§ñ [LLM] Generating response [Validating the need for tool call]\")\n",
    "    response = llm.invoke(new_messages)\n",
    "    \n",
    "    print(\"üì• [LLM] Response from model:\")\n",
    "    response.pretty_print()\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\\nüõÉ [DEBUG] Raw data from model response:\\n\", json.dumps(response.model_dump(), indent=4))\n",
    "    except Exception as e:\n",
    "        print(\"‚õîÔ∏è [ERROR] Error printing JSON response:\", e)\n",
    "\n",
    "    # Check if the response contains a tool call\n",
    "    tool_call = parse_tool_call(response)\n",
    "    if tool_call:\n",
    "        print(\"üîß [LLM] Detected tool call:\")\n",
    "        print(\"   - Tool:\", tool_call.get(\"name\"))\n",
    "        print(\"   - Args:\", tool_call.get(\"args\"))\n",
    "        response.tool_calls = [tool_call]\n",
    "    else:\n",
    "        print(\"üöß [LLM] No tool call detected.\")\n",
    "        response.tool_calls = []\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Generate response using the tool's content\n",
    "def generate(state: MessagesState):\n",
    "    print(\"üõ†Ô∏è [GENERATE] Generating final response using tools:\")\n",
    "    recent_tool_messages = []\n",
    "    \n",
    "    # Iterate through the messages in reverse order to find the most recent tool messages\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "    if not tool_messages:\n",
    "        print(\"‚ùå [ERROR] ToolMessage not found. Cannot generate final response.\")\n",
    "    else:\n",
    "        print(f\"üì¶ [GENERATE] ToolMessages: {len(tool_messages)}\")\n",
    "    \n",
    "    # Construct the system message with the retrieved documents\n",
    "    docs_content = \"\\n\\n\".join(t.content for t in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n",
    "        \"If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "\n",
    "    # Filter the conversation messages to exclude tool calls\n",
    "    conversation_messages = [\n",
    "        m for m in state[\"messages\"]\n",
    "        if m.type in (\"human\", \"system\") or (m.type == \"ai\" and not getattr(m, \"tool_calls\", []))\n",
    "    ]\n",
    "\n",
    "    # Add the system message and conversation messages to the prompt\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    print(\"‚úÖ [FINAL RESPONSE] RAG as tool response:\")\n",
    "    response.pretty_print()\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Build the state graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes and edges\n",
    "builder.add_node(\"query_or_respond\", query_or_respond)\n",
    "tool_node = ToolNode([retrieve])\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.add_node(\"generate\", generate)\n",
    "\n",
    "# Define entry point\n",
    "builder.set_entry_point(\"query_or_respond\")\n",
    "\n",
    "# Define conditional \n",
    "builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "\n",
    "# Define edges\n",
    "builder.add_edge(\"tools\", \"generate\")\n",
    "builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_input = \"O que √© langchain\"\n",
    "    initial_state = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "    \n",
    "    print(\"üöÄ Initializing graph\")\n",
    "\n",
    "    for step in graph.stream(initial_state, stream_mode=\"values\"):\n",
    "        print(\"\\nü¶ú [STEP] New graph step\")\n",
    "        step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
